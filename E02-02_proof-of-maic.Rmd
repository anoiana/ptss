---
params:
  update_date: FALSE
date: "`r source('_supp/helper.R');newdate_func(params$update_date)`"
title: "`r totitle('why does optimization of MAIC method turn out to be fitting logistic model?')`"
output: 
  bookdown::html_document2:
    toc: false
bibliography: ["_supp/citation.bib"]
link-citations: true
---

\newcommand{\bf}[1]{\boldsymbol{#1}}
\newcommand{\hat}[1]{\widehat{#1}}
\newcommand{\mm}[1]{\mathbb{#1}}
\newcommand{\bar}[1]{\overline{#1}}
\newcommand{\tp}[1]{{#1}^{\top}}

\def\E{\Bbb{E}}
\def\V{\Bbb{V}}
\def\P{\Bbb{P}}
\def\I{{\large\unicode{x1D7D9}}}
\def\indep{\perp\!\!\!\!\perp}
\newcommand{\overeq}[2]{\stackrel{#1}{#2}}
\def\epsilon{\varepsilon} 

---

::: {.watermark}
*COMPLETED*
:::

Let $\bf{p} = \tp{(p_1,p_2,\dots,p_n)}$ be propensity score and $\bf X = \tp{(\bf{x}_1,\bf{x}_2, \dots, \bf{x}_n)}$ be baseline characteristics of subjects in IPD. Also, $\bf{\mu}$ denotes average vector of baseline characteristics found in SLD. We aim to estimate $\bf p$ such that $\tp{\bf{p}}\bf{X} = \tp{\bf{\mu}}$, this is, we want to minimize Kullback Leibler divergence of $\bf p$ and empirical function with some constraints. Specifically, we want to minimize 
$$
-\tp{\bf{p}}\ln(\frac{1}{n\bf{p}}),
$$
with two constraints:
$$
\tp{\bf{p}}\bf{1} = 1, \quad and \quad \tp{\bf{p}}\bf{X} = \tp{\bf{\mu}}
$$ 

We shall use *Lagrange Multiplier* to solve such an optimization problem as follows.

$$
\begin{aligned}
\mathcal{L} &= -\tp{\bf{p}}\ln(\frac{1}{n\bf{p}})\bf{1} - \lambda\tp{(\tp{\bf{p}}\bf{X} - \tp{\bf{\mu}})} \\
&= \tp{\bf{p}}\ln(n\bf{p})\bf{1} - \lambda[( \tp{\bf{X}} - \bf{\mu}\tp{\bf{1}} )\bf{p}].
\end{aligned}
(\#eq:eq0)
$$

The solution is to take derivative w.r.t $\bf{p}$ and $\lambda$, equate both derivations to zero and solve both equations. First, Taking derivative $\mathcal{L}$ w.r.t $\lambda$ we obtain 
$$
\tp{\bf{p}}\bf{X} = \tp{\bf{\mu}}.
(\#eq:eq1)
$$
Next, taking derivative w.r.t $p_i$:
$$
\begin{aligned}
&\quad \frac{\partial\mathcal{L}}{\partial p_i} = 0 \\
&\Rightarrow \ln(np_i) +n -\lambda(x_i-\mu) = 0 \\
&\Rightarrow \ln(np_i) = \lambda(x_i-\mu)-n \\
&\Rightarrow p_i = \frac{\exp[\lambda(x_i -\mu)-n]}{n} \\
&\Rightarrow p_i = \frac{e^{-n}}{n}\exp[\lambda(x_i-\mu)]
\end{aligned}
(\#eq:eq2)
$$
thus,
$$
\begin{aligned}
&\quad 1 = \sum_{i=1}^np_i = \frac{e^{-n}}{n}\sum_{i=1}^n\exp[\lambda(x_i-\mu)] \\
& \Rightarrow \frac{n}{e^{-n}} = \sum_{i=1}^n\exp[\lambda(x_i-\mu)]
\end{aligned}
(\#eq:eq3)
$$
substitute `r lb(eq3)` to `r lb(eq2)`, we obtain 

:::{.blackbox .brainstorm}
$$
p_i = \frac{\exp[\lambda(x_i - \mu)]}{\sum_{i=1}^n\exp[\lambda(x_i-\mu)]}
(\#eq:eq4)
$$
:::
`r lb(eq4)` indicates that $p_i$ can be estimated by fitting logistic model in which $z_i= x_i-\mu$ is one covariate, and $z_i$ must be statisfying `r lb(eq1)`.


